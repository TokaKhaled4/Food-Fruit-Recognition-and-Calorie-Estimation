{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "129b1b05",
   "metadata": {},
   "source": [
    "# **Imports & Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06875728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== IMPORTS & CONFIG ========================\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "# from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
    "from tensorflow.keras.layers import UnitNormalization\n",
    "\n",
    "# ================= CONFIG =================\n",
    "TEST_IMAGES_FOLDER = \"TestCases/Integerated Test\"\n",
    "OUTPUT_ROOT = \"Integrated_Test_Output\"\n",
    "\n",
    "# ---------- Stage 1: Food vs Fruit ----------\n",
    "FOOD_FRUIT_MODEL_PATH = \"Models/part_a_best_mobilenet.pth\"\n",
    "IMG_SIZE_FF = 224\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- Food Recognition (Siamese) ----------\n",
    "SIAMESE_MODEL_PATH = \"Models/PartB_EfficientNetB2.h5\"\n",
    "TRAIN_DIR_FOOD = \"Project Data/Food/Train\"\n",
    "VALID_DIR_FOOD = \"Project Data/Food/Validation\"\n",
    "\n",
    "\n",
    "IMG_SIZE_SIAMESE = (300, 300)\n",
    "SAMPLES_PER_CLASS = 10\n",
    "\n",
    "# ---------- Fruit Classification ----------\n",
    "FRUIT_MODEL_PATH = \"Models/MobileNetV2_PartC.keras\"\n",
    "TRAIN_DIR_FRUIT = \"Project Data/Fruit/Train\"\n",
    "IMG_SIZE_FRUIT = 350\n",
    "\n",
    "# ---------- Calories (SEPARATE FILES) ----------\n",
    "FOOD_CALORIES_FILE_TRAIN = \"Project Data/Food/Train Calories.txt\"\n",
    "FOOD_CALORIES_FILE_VALID = \"Project Data/Food/Val Calories.txt\"\n",
    "FRUIT_CALORIES_FILE = \"Project Data/Fruit/Calories.txt\"\n",
    "\n",
    "# ---------- Binary Segmentation ----------\n",
    "SEG_MODEL_PATH = \"Models/segnet_best.keras\"\n",
    "SEG_IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# ---------- Multi Segmentation ----------\n",
    "MULTI_SEG_MODEL_PATH = \"Models/best_multiclass_resnet50_unet.keras\"\n",
    "CLASS_MAPPING_FILE = \"Models/class_mapping.json\"\n",
    "COLOR_MAPPING_FILE = \"Models/color_mapping.json\"\n",
    "MULTI_SEG_IMG_SIZE = 224\n",
    "NUM_CLASSES = 31\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c444609d",
   "metadata": {},
   "source": [
    "# **Food VS Fruit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a1cca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodFruitClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mobilenet = models.mobilenet_v2(weights=None)\n",
    "        self.mobilenet.classifier[1] = nn.Linear(\n",
    "            self.mobilenet.classifier[1].in_features, 2\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mobilenet(x)\n",
    "\n",
    "\n",
    "food_fruit_model = FoodFruitClassifier()\n",
    "\n",
    "checkpoint = torch.load(FOOD_FRUIT_MODEL_PATH, map_location=DEVICE)\n",
    "\n",
    "# ✅ This will now load correctly\n",
    "food_fruit_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "food_fruit_model.to(DEVICE)\n",
    "food_fruit_model.eval()\n",
    "\n",
    "transform_ff = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE_FF, IMG_SIZE_FF)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "def predict_food_or_fruit(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img = transform_ff(img).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = food_fruit_model(img)\n",
    "        pred = torch.argmax(out, dim=1).item()\n",
    "    return \"Food\" if pred == 0 else \"Fruit\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d744f6cc",
   "metadata": {},
   "source": [
    "# **Food Recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e530630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 98 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001246A67F9C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 98 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001246A67F9C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# # Load Siamese Encoder\n",
    "encoder = load_model(SIAMESE_MODEL_PATH)\n",
    "\n",
    "def get_embedding(img_path):\n",
    "    img = load_img(img_path, target_size=IMG_SIZE_SIAMESE)\n",
    "    img = img_to_array(img)\n",
    "    img = preprocess_input(np.expand_dims(img, axis=0))\n",
    "    return encoder.predict(img, verbose=0)[0]\n",
    "\n",
    "# Load representatives from Train + Valid, one per class\n",
    "def load_food_representatives(dirs=[VALID_DIR_FOOD, TRAIN_DIR_FOOD], samples_per_class=SAMPLES_PER_CLASS):\n",
    "    reps = {}\n",
    "    for d in dirs:\n",
    "        if not os.path.exists(d):\n",
    "            continue\n",
    "        for cls in os.listdir(d):\n",
    "            cls_path = os.path.join(d, cls)\n",
    "            if not os.path.isdir(cls_path) or cls in reps:\n",
    "                continue  # skip if already added\n",
    "            imgs = glob(os.path.join(cls_path, \"*.jpg\"))[:samples_per_class]\n",
    "            if imgs:\n",
    "                reps[cls] = [get_embedding(imgs[0])]\n",
    "    return reps\n",
    "\n",
    "food_embeddings = load_food_representatives()\n",
    "\n",
    "def recognize_food(img_path):\n",
    "    emb = get_embedding(img_path)\n",
    "    distances = {cls: np.mean([np.linalg.norm(emb - e) for e in embs])\n",
    "                 for cls, embs in food_embeddings.items()}\n",
    "    return min(distances, key=distances.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3674ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import open_clip\n",
    "# from PIL import Image\n",
    "\n",
    "# # Load your fine-tuned CLIP model\n",
    "# FINETUNED_MODEL_PATH = \"Models/clip_finetuned_food_improved.pth\"\n",
    "# clip_model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "#     model_name=\"ViT-B-32\",\n",
    "#     pretrained=\"openai\"\n",
    "# )\n",
    "\n",
    "# checkpoint = torch.load(FINETUNED_MODEL_PATH, map_location=DEVICE)\n",
    "# clip_model.load_state_dict(checkpoint[\"clip_model_state_dict\"])\n",
    "\n",
    "# clip_model = clip_model.to(DEVICE).eval()\n",
    "# print(\"✓ CLIP model loaded successfully!\")\n",
    "\n",
    "# # Load Food classes from Train + Valid\n",
    "# food_classes = sorted(\n",
    "#     set(os.listdir(TRAIN_DIR_FOOD)) | set(os.listdir(VALID_DIR_FOOD))\n",
    "# )\n",
    "\n",
    "# def recognize_food_clip(img_path, classes=food_classes):\n",
    "#     \"\"\"Recognize food class using fine-tuned CLIP\"\"\"\n",
    "#     img = preprocess(Image.open(img_path)).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         img_features = clip_model.encode_image(img)\n",
    "#         img_features /= img_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#         # Tokenize text labels\n",
    "#         text_inputs = open_clip.tokenize(classes).to(DEVICE)\n",
    "#         text_features = clip_model.encode_text(text_inputs)\n",
    "#         text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#         # Compute cosine similarity\n",
    "#         similarity = (100.0 * img_features @ text_features.T).softmax(dim=-1)\n",
    "#         class_idx = similarity.argmax().item()\n",
    "        \n",
    "#     return classes[class_idx]\n",
    "\n",
    "\n",
    "# len(food_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "152bf74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import open_clip\n",
    "# from PIL import Image\n",
    "\n",
    "# # Load your fine-tuned CLIP model\n",
    "# FINETUNED_MODEL_PATH = \"Models/clip_finetuned_5_Shots.pth\"\n",
    "# # =========================================================\n",
    "# # LOAD CLIP MODEL\n",
    "# # =========================================================\n",
    "# clip_model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "#     model_name=\"ViT-B-32\",\n",
    "#     pretrained=\"openai\"\n",
    "# )\n",
    "\n",
    "# checkpoint = torch.load(FINETUNED_MODEL_PATH, map_location=DEVICE)\n",
    "# clip_model.load_state_dict(checkpoint[\"clip_model_state_dict\"])\n",
    "\n",
    "# clip_model = clip_model.to(DEVICE).eval()\n",
    "# print(\"✓ CLIP model loaded successfully\")\n",
    "\n",
    "# # =========================================================\n",
    "# # CLIP IMAGE EMBEDDING\n",
    "# # =========================================================\n",
    "# def get_clip_embedding(img_path):\n",
    "#     img = preprocess(Image.open(img_path).convert(\"RGB\")) \\\n",
    "#             .unsqueeze(0).to(DEVICE)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         emb = clip_model.encode_image(img)\n",
    "#         emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#     return emb\n",
    "\n",
    "# # =========================================================\n",
    "# # LOAD FOOD REPRESENTATIVES (TRAIN + VALID)\n",
    "# # =========================================================\n",
    "# def load_food_representatives(\n",
    "#     dirs=[VALID_DIR_FOOD, TRAIN_DIR_FOOD],\n",
    "#     samples_per_class=5\n",
    "# ):\n",
    "#     reps = {}\n",
    "\n",
    "#     for d in dirs:\n",
    "#         if not os.path.exists(d):\n",
    "#             continue\n",
    "\n",
    "#         for cls in os.listdir(d):\n",
    "#             cls_path = os.path.join(d, cls)\n",
    "\n",
    "#             if not os.path.isdir(cls_path) or cls in reps:\n",
    "#                 continue\n",
    "\n",
    "#             imgs = glob(os.path.join(cls_path, \"*.jpg\"))[:samples_per_class]\n",
    "\n",
    "#             if imgs:\n",
    "#                 reps[cls] = [get_clip_embedding(img) for img in imgs]\n",
    "\n",
    "#     return reps\n",
    "\n",
    "\n",
    "# print(\"Loading food representatives...\")\n",
    "# food_reps = load_food_representatives(\n",
    "#     dirs=[TRAIN_DIR_FOOD, VALID_DIR_FOOD],\n",
    "#     samples_per_class=5\n",
    "# )\n",
    "# print(f\"✓ Loaded {len(food_reps)} food classes\")\n",
    "\n",
    "# # =========================================================\n",
    "# # FOOD RECOGNITION (IMAGE → IMAGE)\n",
    "# # =========================================================\n",
    "# def recognize_food_clip(img_path):\n",
    "#     \"\"\"\n",
    "#     Image-only food recognition using CLIP representatives\n",
    "#     Returns: food class (string)\n",
    "#     \"\"\"\n",
    "#     query_emb = get_clip_embedding(img_path)\n",
    "\n",
    "#     best_cls = None\n",
    "#     best_score = -1\n",
    "\n",
    "#     for cls, emb_list in food_reps.items():\n",
    "#         for emb in emb_list:\n",
    "#             score = (query_emb @ emb.T).item()\n",
    "#             if score > best_score:\n",
    "#                 best_score = score\n",
    "#                 best_cls = cls\n",
    "\n",
    "#     return best_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297549a1",
   "metadata": {},
   "source": [
    "# **Fruit Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f950000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= FRUIT CLASSIFICATION =================\n",
    "fruit_model = tf.keras.models.load_model(FRUIT_MODEL_PATH)\n",
    "\n",
    "fruit_classes = sorted([\n",
    "    d for d in os.listdir(TRAIN_DIR_FRUIT)\n",
    "    if os.path.isdir(os.path.join(TRAIN_DIR_FRUIT, d))\n",
    "])\n",
    "\n",
    "def preprocess_fruit(img_path):\n",
    "    img = image.load_img(img_path, target_size=(IMG_SIZE_FRUIT, IMG_SIZE_FRUIT))\n",
    "    img = image.img_to_array(img) / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "def recognize_fruit(img_path):\n",
    "    pred = fruit_model.predict(preprocess_fruit(img_path), verbose=0)\n",
    "    return fruit_classes[np.argmax(pred)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cb40ae",
   "metadata": {},
   "source": [
    "# **Calculating Calories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3c4b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_calories(file_path):\n",
    "    calories = {}\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            m = re.match(r\"(.+?):\\s*~?([\\d.]+)\", line.strip())\n",
    "            if m:\n",
    "                # normalize class names: lowercase + underscores\n",
    "                cls_name = m.group(1).strip().lower().replace(\" \", \"_\")\n",
    "                calories[cls_name] = float(m.group(2))\n",
    "    return calories\n",
    "\n",
    "# Merge food calories from train + valid\n",
    "food_calories = {}\n",
    "food_calories.update(load_calories(FOOD_CALORIES_FILE_TRAIN))\n",
    "food_calories.update(load_calories(FOOD_CALORIES_FILE_VALID))\n",
    "\n",
    "# Load fruit calories\n",
    "fruit_calories = load_calories(FRUIT_CALORIES_FILE)\n",
    "\n",
    "def extract_grams(name):\n",
    "    return int(re.search(r\"(\\d+)g\", name).group(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2090e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(food_calories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cebde74",
   "metadata": {},
   "source": [
    "# **Binary Segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1df60f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model = tf.keras.models.load_model(SEG_MODEL_PATH, compile=False)\n",
    "\n",
    "def run_binary_segmentation(img_path, save_dir):\n",
    "    filename = os.path.basename(img_path)\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error reading {filename}\")\n",
    "        return\n",
    "    h, w = img.shape[:2]\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img_rgb, SEG_IMAGE_SIZE)\n",
    "    img_input = np.expand_dims(img_resized.astype(\"float32\") / 255.0, axis=0)\n",
    "    mask = np.squeeze(seg_model.predict(img_input, verbose=0))\n",
    "    mask_binary = (mask > 0.5).astype(np.uint8)\n",
    "    mask_resized = cv2.resize(mask_binary, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "    mask_final = mask_resized * 255\n",
    "    save_path = os.path.join(save_dir, os.path.splitext(filename)[0] + \"_mask.png\")\n",
    "    cv2.imwrite(save_path, mask_final)\n",
    "    print(f\"Saved segmentation mask: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26aa21c",
   "metadata": {},
   "source": [
    "# **Multi Segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b972096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== IMPORTS ========================\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from skimage import measure\n",
    "from scipy import stats\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "import json\n",
    "import time\n",
    "\n",
    "# ======================== CONFIG ========================\n",
    "MULTI_SEG_MODEL_PATH = \"Models/best_multiclass_resnet50_unet.keras\"\n",
    "CLASS_MAPPING_FILE = \"Models/class_mapping.json\"\n",
    "COLOR_MAPPING_FILE = \"Models/color_mapping.json\"\n",
    "MULTI_SEG_IMG_SIZE = 224\n",
    "\n",
    "# Load class/color mappings\n",
    "with open(CLASS_MAPPING_FILE, 'r') as f:\n",
    "    class_mapping = json.load(f)\n",
    "with open(COLOR_MAPPING_FILE, 'r') as f:\n",
    "    color_mapping = json.load(f)\n",
    "reverse_mapping = {v: k for k, v in class_mapping.items()}\n",
    "\n",
    "# ======================== CUSTOM OBJECTS ========================\n",
    "@register_keras_serializable()\n",
    "class MultiClassDiceCoefficient(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes=31, name='dice_coefficient', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.total_dice = self.add_weight(name='total_dice', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        y_true = tf.squeeze(y_true, axis=-1)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "        dice_sum = 0.0\n",
    "        valid_classes = 0.0\n",
    "\n",
    "        for i in range(self.num_classes):\n",
    "            class_idx = tf.cast(i, tf.float32)\n",
    "            y_t = tf.cast(tf.equal(y_true, class_idx), tf.float32)\n",
    "            y_p = tf.cast(tf.equal(y_pred, class_idx), tf.float32)\n",
    "\n",
    "            inter = tf.reduce_sum(y_t * y_p)\n",
    "            union = tf.reduce_sum(y_t) + tf.reduce_sum(y_p)\n",
    "            present = tf.cast(tf.reduce_sum(y_t) > 0, tf.float32)\n",
    "\n",
    "            dice = tf.math.divide_no_nan(2.0 * inter, union)\n",
    "            dice_sum += dice * present\n",
    "            valid_classes += present\n",
    "\n",
    "        batch_dice = tf.math.divide_no_nan(dice_sum, valid_classes)\n",
    "        self.total_dice.assign_add(batch_dice)\n",
    "        self.count.assign_add(1.0)\n",
    "\n",
    "    def result(self):\n",
    "        return tf.math.divide_no_nan(self.total_dice, self.count)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_dice.assign(0.0)\n",
    "        self.count.assign(0.0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_classes\": self.num_classes})\n",
    "        return config\n",
    "\n",
    "@register_keras_serializable()\n",
    "def combined_loss(y_true, y_pred):\n",
    "    ce_loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred))\n",
    "    return ce_loss\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred)\n",
    "\n",
    "# ======================== HELPER FUNCTIONS ========================\n",
    "def preprocess_image(img_rgb, img_size):\n",
    "    img = tf.convert_to_tensor(img_rgb)\n",
    "    img = tf.image.resize(img, (img_size, img_size))\n",
    "    img = preprocess_input(img)\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "def clean_mask_advanced(pred_mask):\n",
    "    if hasattr(pred_mask, 'numpy'):\n",
    "        pred_mask = pred_mask.numpy()\n",
    "    pred_mask = np.array(pred_mask)\n",
    "    binary_mask = (pred_mask > 0).astype(np.uint8)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\n",
    "    binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n",
    "    labels = measure.label(binary_mask)\n",
    "    cleaned_mask = pred_mask.copy()\n",
    "    for region in measure.regionprops(labels):\n",
    "        if region.area < 50:\n",
    "            coords = region.coords\n",
    "            cleaned_mask[coords[:,0], coords[:,1]] = 0\n",
    "            continue\n",
    "        coords = region.coords\n",
    "        region_vals = pred_mask[coords[:,0], coords[:,1]]\n",
    "        region_vals = region_vals[region_vals > 0]\n",
    "        if len(region_vals) > 0:\n",
    "            most_frequent_class = stats.mode(region_vals, keepdims=True)[0][0]\n",
    "            cleaned_mask[coords[:,0], coords[:,1]] = most_frequent_class\n",
    "    return cleaned_mask\n",
    "\n",
    "def run_multiclass_segmentation(img_path, save_dir):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Could not read {img_path}\")\n",
    "        return\n",
    "    h, w = img.shape[:2]\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    inp = preprocess_image(img_rgb, MULTI_SEG_IMG_SIZE)\n",
    "\n",
    "    start_time = time.time()\n",
    "    pred = multi_seg_model.predict(inp, verbose=0)[0]\n",
    "    inference_time = (time.time() - start_time) * 1000\n",
    "\n",
    "    raw_mask = np.argmax(pred, axis=-1)\n",
    "    mask_clean = clean_mask_advanced(raw_mask)\n",
    "\n",
    "    total_pixels = MULTI_SEG_IMG_SIZE * MULTI_SEG_IMG_SIZE\n",
    "    detected_indices = np.unique(mask_clean)\n",
    "    detected_indices = detected_indices[detected_indices != 0]\n",
    "\n",
    "    for idx in detected_indices:\n",
    "        pixel_count = np.sum(mask_clean == idx)\n",
    "        percentage = (pixel_count / total_pixels) * 100\n",
    "        if percentage <= 1.0:\n",
    "            mask_clean[mask_clean == idx] = 0\n",
    "\n",
    "# Create colored mask\n",
    "    colored = np.zeros((MULTI_SEG_IMG_SIZE, MULTI_SEG_IMG_SIZE, 3), dtype=np.uint8)\n",
    "    class_names_detected = []\n",
    "    for idx in detected_indices:\n",
    "        cls_name = reverse_mapping.get(int(idx), \"background\")\n",
    "        rgb_color = np.array(color_mapping.get(cls_name, [0,0,0]), dtype=np.uint8)\n",
    "        bgr_color = rgb_color[::-1]  # Convert RGB to BGR for OpenCV\n",
    "        colored[mask_clean == idx] = bgr_color\n",
    "        class_names_detected.append(cls_name)\n",
    "\n",
    "    # Resize to original size\n",
    "    colored = cv2.resize(colored, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Create header above the image\n",
    "    header_height = 30 + 20*len(class_names_detected)\n",
    "    result_img = np.zeros((h + header_height, w, 3), dtype=np.uint8)\n",
    "\n",
    "    # Fill header with black background\n",
    "    result_img[:header_height, :, :] = 0\n",
    "\n",
    "    # Write class names\n",
    "    for i, cls_name in enumerate(class_names_detected):\n",
    "        cv2.putText(\n",
    "            result_img,\n",
    "            cls_name,\n",
    "            (10, 25 + i*20),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7,\n",
    "            (255, 255, 255),\n",
    "            2\n",
    "        )\n",
    "\n",
    "    # Place the colored mask below header\n",
    "    result_img[header_height:, :, :] = colored\n",
    "\n",
    "    # Save result\n",
    "    base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    save_path = os.path.join(save_dir, f\"{base}_multiseg_mask.png\")\n",
    "    cv2.imwrite(save_path, result_img)\n",
    "\n",
    "    return result_img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ======================== LOAD MODEL ========================\n",
    "multi_seg_model = tf.keras.models.load_model(\n",
    "    MULTI_SEG_MODEL_PATH,\n",
    "    custom_objects={\n",
    "        \"combined_loss\": combined_loss,\n",
    "        \"dice_loss\": dice_loss,\n",
    "        \"MultiClassDiceCoefficient\": MultiClassDiceCoefficient\n",
    "    },\n",
    "    compile=False\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d57358",
   "metadata": {},
   "source": [
    "# **Main Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e2b2472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img1_75g.jpg → Food | creme_brulee | 262.50 kcal\n",
      "img2_280g.jpg → Food | hot_and_sour_soup | 140.00 kcal\n",
      "Saved segmentation mask: Integrated_Test_Output\\img3_300g\\img3_300g_mask.png\n",
      "img3_300g.jpg → Fruit | Mango_Amrapali | 195.00 kcal\n",
      "Saved segmentation mask: Integrated_Test_Output\\img4_350g\\img4_350g_mask.png\n",
      "img4_350g.jpg → Fruit | Elephant Apple | 210.00 kcal\n",
      "img5_700g.jpg → Food | creme_brulee | 2450.00 kcal\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "\n",
    "for img_name in os.listdir(TEST_IMAGES_FOLDER):\n",
    "    if not img_name.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "        continue\n",
    "\n",
    "    img_path = os.path.join(TEST_IMAGES_FOLDER, img_name)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, os.path.splitext(img_name)[0])\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    main_class = predict_food_or_fruit(img_path)\n",
    "\n",
    "    if main_class == \"Food\":\n",
    "        sub = recognize_food(img_path)\n",
    "        # sub = recognize_food_clip(img_path)\n",
    "        cal = food_calories.get(sub.lower().replace(\" \", \"_\"), 0)\n",
    "    else:\n",
    "        sub = recognize_fruit(img_path)\n",
    "        cal = fruit_calories.get(sub.lower().replace(\" \", \"_\"), 0)\n",
    "\n",
    "        # Save BOTH masks for Fruit\n",
    "        run_binary_segmentation(img_path, out_dir)\n",
    "        run_multiclass_segmentation(img_path, out_dir)\n",
    "\n",
    "    grams = extract_grams(img_name)\n",
    "    total_cal = grams * cal\n",
    "\n",
    "    with open(os.path.join(out_dir, \"result.txt\"), \"w\") as f:\n",
    "        f.write(f\"{main_class}\\n{sub}\\n{total_cal:.2f}\\n\")\n",
    "\n",
    "    print(f\"{img_name} → {main_class} | {sub} | {total_cal:.2f} kcal\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
